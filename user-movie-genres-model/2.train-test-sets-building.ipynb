{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from spark import SparkSessionFactory, read_csv, write_csv, column_values, train_test_split, shuffle_df, Column\n",
    "from util import remove_dir, Config, LoggerFactory\n",
    "\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql as s\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_counts(train_set, val_set, test_set):\n",
    "    logging.info(f'Train set count = {train_set.count()} observations.')\n",
    "    logging.info(f'Validation set count = {val_set.count()} observations.')\n",
    "    logging.info(f'Test set count = {test_set.count()} observations.')\n",
    "\n",
    "def show(df, limit=5): return df.limit(limit).toPandas()\n",
    "\n",
    "def show_columns(df, columns=[], limit=5): return show(df.select(*[f.col(c) for c in columns]), limit)\n",
    "\n",
    "def show_counts(df, columns=[]):\n",
    "    logging.info('Count:')\n",
    "    for column in columns:\n",
    "        logging.info(f'- {column}: {df.select(column).distinct().count()}')\n",
    "        \n",
    "class TrainUserMovieFilter:\n",
    "    def __init__(self, train_set):\n",
    "        self.__train_user_seqs = train_set.select('user_seq').distinct().rdd.map(lambda r: r.user_seq).collect()\n",
    "        self.__train_movie_seqs = train_set.select('movie_seq').distinct().rdd.map(lambda r: r.movie_seq).collect()\n",
    "\n",
    "    def perform(self, obs_set):\n",
    "        obs_set2 = obs_set.filter(obs_set['user_seq'].isin(self.__train_user_seqs))\n",
    "        obs_set3 = obs_set2.filter(obs_set2['movie_seq'].isin(self.__train_movie_seqs))\n",
    "\n",
    "        logging.info(f'Excluded users: {abs(obs_set.count() - obs_set2.count())}')\n",
    "        logging.info(f'Excluded movies: {abs(obs_set2.count() - obs_set3.count())}')\n",
    "        \n",
    "        return obs_set3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(path='../config/config.yaml')\n",
    "LoggerFactory(config['logger']).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Create a predefined spack session. this is used to create a pipeline that build the model input features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://skynet:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>recommendations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7c9df06c10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = SparkSessionFactory.create()\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1646082088382'),\n",
       " ('spark.driver.memory', '16G'),\n",
       " ('spark.executor.instances', '12'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/adrian/development/machine-learning/recommendations/user-movie-genres-model/spark-warehouse'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'recommendations'),\n",
       " ('spark.driver.port', '42073'),\n",
       " ('spark.driver.host', 'skynet'),\n",
       " ('spark.executor.memory', '16G'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.startTime', '1646082087965'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When create a spark session this run a new spark cluster with one instance in localhost. You can monitor instance jobs clicking the **Spark UI** link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Load raw dataset to a spark dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_PATH = './temp'\n",
    "DATASET_PATH = f'{TEMP_PATH}/dataset'\n",
    "TRAIN_PATH = f'{TEMP_PATH}/train'\n",
    "VAL_PATH = f'{TEMP_PATH}/val'\n",
    "TEST_PATH = f'{TEMP_PATH}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = shuffle_df(read_csv(session, f'{DATASET_PATH}/*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Get all users and movies ids and let's see how many elements has each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:01:32 INFO Count:\n",
      "2022-02-28 18:01:32 INFO - user_id: 610\n",
      "2022-02-28 18:01:33 INFO - movie_id: 9724\n"
     ]
    }
   ],
   "source": [
    "show_counts(dataset, ['user_id', 'movie_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Let's see all dataset columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rating',\n",
       " 'user_id',\n",
       " 'movie_id',\n",
       " 'gen_comedy',\n",
       " 'gen_drama',\n",
       " 'gen_romance',\n",
       " 'gen_action',\n",
       " 'gen_adventure',\n",
       " 'gen_sci_fi',\n",
       " 'gen_crime',\n",
       " 'gen_thriller',\n",
       " 'gen_war',\n",
       " 'gen_documentary',\n",
       " 'gen_mystery',\n",
       " 'gen_imax',\n",
       " 'gen_horror',\n",
       " 'gen_children',\n",
       " 'gen_fantasy',\n",
       " 'gen_animation',\n",
       " 'gen_musical',\n",
       " 'gen_film_noir',\n",
       " 'gen_western',\n",
       " 'gen_none']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: add user an movies sequence index/new ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_part, _ = train_test_split(dataset, test_size=0.8)\n",
    "dataset_part = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_part2 = Column.sequence(session, dataset_part, 'user_id', 'user_seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_part3 = Column.sequence(session, dataset_part2, 'movie_id', 'movie_seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:01:44 INFO Count:\n",
      "2022-02-28 18:01:44 INFO - user_id: 610\n",
      "2022-02-28 18:01:44 INFO - user_seq: 610\n",
      "2022-02-28 18:01:46 INFO - movie_id: 9724\n",
      "2022-02-28 18:01:46 INFO - movie_seq: 9724\n"
     ]
    }
   ],
   "source": [
    "show_counts(dataset_part3, ['user_id', 'user_seq', 'movie_id', 'movie_seq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Split data into train, validacion, test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:01:47 INFO Train set count = 70691 observations.\n",
      "2022-02-28 18:01:47 INFO Validation set count = 21115 observations.\n",
      "2022-02-28 18:01:47 INFO Test set count = 9030 observations.\n"
     ]
    }
   ],
   "source": [
    "train_set, val_test_sets = train_test_split(dataset_part3, test_size=0.3)\n",
    "val_set, test_set = train_test_split(val_test_sets, test_size=0.3)\n",
    "\n",
    "log_counts(train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6**: Get only test and val samples for users and movies that appears in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:01:53 INFO Excluded users: 0\n",
      "2022-02-28 18:01:54 INFO Excluded movies: 985\n",
      "2022-02-28 18:01:59 INFO Excluded users: 0\n",
      "2022-02-28 18:02:00 INFO Excluded movies: 407\n",
      "2022-02-28 18:02:01 INFO Train set count = 70691 observations.\n",
      "2022-02-28 18:02:01 INFO Validation set count = 20130 observations.\n",
      "2022-02-28 18:02:02 INFO Test set count = 8623 observations.\n"
     ]
    }
   ],
   "source": [
    "filter = TrainUserMovieFilter(train_set)\n",
    "\n",
    "val_set2 = filter.perform(val_set)\n",
    "test_set2 = filter.perform(test_set)\n",
    "\n",
    "log_counts(train_set, val_set2, test_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./temp/test'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_dir(TRAIN_PATH)\n",
    "remove_dir(VAL_PATH)\n",
    "remove_dir(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(train_set, TRAIN_PATH)\n",
    "write_csv(val_set2, VAL_PATH)\n",
    "write_csv(test_set2, TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
